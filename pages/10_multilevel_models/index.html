<!doctype html> <html lang=en > <meta charset=UTF-8 > <meta name=viewport  content="width=device-width, initial-scale=1"> <link rel=stylesheet  href="/Bayesian-Julia/libs/katex/katex.min.css"> <link rel=stylesheet  href="/Bayesian-Julia/libs/highlight/github.min.css"> <link rel=stylesheet  href="/Bayesian-Julia/css/jtd.css"> <link rel=icon  href="/Bayesian-Julia/assets/favicon.ico"> <title>Multilevel Models &#40;a.k.a. Hierarchical Models&#41;</title> <div class=page-wrap > <div class=side-bar > <div class=header > <a href="/Bayesian-Julia/" class=title > Bayesian Stats </a> </div> <label for=show-menu  class=show-menu >MENU</label> <input type=checkbox  id=show-menu  role=button > <div class=menu  id=side-menu > <ul class=menu-list > <li class="menu-list-item "><a href="/Bayesian-Julia/" class="menu-list-link ">Home</a> <li class="menu-list-item "><a href="/Bayesian-Julia/pages/1_why_Julia/" class="menu-list-link ">1. Why Julia?</a> <li class="menu-list-item "><a href="/Bayesian-Julia/pages/2_bayes_stats/" class="menu-list-link ">2. What is Bayesian Statistics?</a> <li class="menu-list-item "><a href="/Bayesian-Julia/pages/3_prob_dist/" class="menu-list-link ">3. Common Probability Distributions</a> <li class="menu-list-item "><a href="/Bayesian-Julia/pages/4_Turing/" class="menu-list-link ">4. How to use Turing</a> <li class="menu-list-item "><a href="/Bayesian-Julia/pages/5_MCMC/" class="menu-list-link ">5. Markov Chain Monte Carlo (MCMC)</a> <li class="menu-list-item "><a href="/Bayesian-Julia/pages/6_linear_reg/" class="menu-list-link ">6. Bayesian Linear Regression</a> <li class="menu-list-item "><a href="/Bayesian-Julia/pages/7_logistic_reg/" class="menu-list-link ">7. Bayesian Logistic Regression</a> <li class="menu-list-item "><a href="/Bayesian-Julia/pages/8_count_reg/" class="menu-list-link ">8. Bayesian Regression with Count Data</a> <li class="menu-list-item "><a href="/Bayesian-Julia/pages/9_robust_reg/" class="menu-list-link ">9. Robust Bayesian Regression</a> <li class="menu-list-item active"><a href="/Bayesian-Julia/pages/10_multilevel_models/" class="menu-list-link active">10. Multilevel Models</a> <li class="menu-list-item "><a href="/Bayesian-Julia/pages/11_Turing_tricks/" class="menu-list-link ">11. Computational Tricks with Turing</a> <li class="menu-list-item "><a href="/Bayesian-Julia/pages/12_epi_models/" class="menu-list-link ">12. Bayesian Epidemiological Models</a> </ul> </div> <div class=footer > <a href="https://www.julialang.org"><img style="height:50px;padding-left:10px;margin-bottom:15px;" src="https://julialang.org/assets/infra/logo.svg" alt="Julia Logo"></a> </div> </div> <div class=main-content-wrap > <div class=main-content > <div class=main-header > <a id=github  href="https://github.com/storopoli/Bayesian-Julia">Code on GitHub</a> </div> <script async src="https://www.googletagmanager.com/gtag/js?id=UA-186284914-6"></script> <script> window.dataLayer = window.dataLayer || []; function gtag(){dataLayer.push(arguments);} gtag('js', new Date()); gtag('config', 'UA-186284914-6'); </script> <div class=franklin-content ><div class=franklin-toc ><ol><li><a href="#when_to_use_multilevel_models">When to use Multilevel Models?</a><li><a href="#hyperpriors">Hyperpriors</a><li><a href="#three_approaches_to_multilevel_models">Three Approaches to Multilevel Models</a><ol><li><a href="#random-intercept_model">Random-Intercept Model</a><li><a href="#random-slope_model">Random-Slope Model</a><li><a href="#random-intercept-slope_model">Random-Intercept-Slope Model</a></ol><li><a href="#example_-_cheese_ratings">Example - Cheese Ratings</a><li><a href="#references">References</a></ol></div> <h1 id=multilevel_models_aka_hierarchical_models ><a href="#multilevel_models_aka_hierarchical_models" class=header-anchor >Multilevel Models &#40;a.k.a. Hierarchical Models&#41;</a></h1> <p>Bayesian hierarchical models &#40;also called multilevel models&#41; are a statistical model written at <em>multiple</em> levels &#40;hierarchical form&#41; that estimates the parameters of the posterior distribution using the Bayesian approach. The sub-models combine to form the hierarchical model, and Bayes&#39; theorem is used to integrate them with the observed data and to account for all the uncertainty that is present. The result of this integration is the posterior distribution, also known as an updated probability estimate, as additional evidence of the likelihood function is integrated together with the prior distribution of the parameters.</p> <p>Hierarchical modeling is used when information is available at several different levels of observation units. The hierarchical form of analysis and organization helps to understand multiparameter problems and also plays an important role in the development of computational strategies.</p> <p>Hierarchical models are mathematical statements that involve several parameters, so that the estimates of some parameters depend significantly on the values of other parameters. The figure below shows a hierarchical model in which there is a \(\phi\) hyperparameter that parameterizes the parameters \(\theta_1, \theta_2, \dots, \theta_N\) that are finally used to infer the posterior density of some variable of interest \(\mathbf{y} = y_1, y_2, \dots, y_N\).</p> <p><img src="/Bayesian-Julia/pages/images/hierarchical.png" alt="Bayesian Workflow" /></p> <p><div class=text-center ><em>Hierarchical Model</em></div> <br/></p> <h2 id=when_to_use_multilevel_models ><a href="#when_to_use_multilevel_models" class=header-anchor >When to use Multilevel Models?</a></h2> <p>Multilevel models are particularly suitable for research projects where participant data is organized at more than one level, <em>i.e.</em> nested data. Units of analysis are usually individuals &#40;at a lower level&#41; that are nested in contextual/aggregate units &#40;at a higher level&#41;. An example is when we are measuring the performance of individuals and we have additional information about belonging to different groups such as sex, age group, hierarchical level, educational level or housing status.</p> <p>There is a main assumption that cannot be violated in multilevel models which is <strong>exchangeability</strong> &#40;de Finetti, 1974; Nau, 2001&#41;. Yes, this is the same assumption that we discussed in <a href="/Bayesian-Julia/pages/2_bayes_stats/">2. <strong>What is Bayesian Statistics?</strong></a>. This assumption assumes that groups are exchangeable. The figure below shows a graphical representation of the exchangeability. The groups shown as &quot;cups&quot; that contain observations shown as &quot;balls&quot;. If in the model&#39;s inferences, this assumption is violated, then multilevel models are not appropriate. This means that, since there is no theoretical justification to support exchangeability, the inferences of the multilevel model are not robust and the model can suffer from several pathologies and should not be used for any scientific or applied analysis.</p> <p><img src="/Bayesian-Julia/pages/images/exchangeability-1.png" alt="Bayesian Workflow" /> <img src="/Bayesian-Julia/pages/images/exchangeability-2.png" alt="Bayesian Workflow" /></p> <p><div class=text-center ><em>Exchangeability – Images from <a href="https://betanalpha.github.io/">Michael Betancourt</a></em></div> <br/></p> <h2 id=hyperpriors ><a href="#hyperpriors" class=header-anchor >Hyperpriors</a></h2> <p>As the priors of the parameters are sampled from another prior of the hyperparameter &#40;upper-level&#39;s parameter&#41;, which are called <strong>hyperpriors</strong>. This makes one group&#39;s estimates help the model to better estimate the other groups by providing more <strong>robust and stable estimates</strong>.</p> <p>We call the global parameters as <strong>population effects</strong> &#40;or population-level effects, also sometimes called fixed effects&#41; and the parameters of each group as <strong>group effects</strong> &#40;or group-level effects, also sometimes called random effects&#41;. That is why multilevel models are also known as mixed models in which we have both fixed effects and random effects.</p> <h2 id=three_approaches_to_multilevel_models ><a href="#three_approaches_to_multilevel_models" class=header-anchor >Three Approaches to Multilevel Models</a></h2> <p>Multilevel models generally fall into three approaches:</p> <ol> <li><p><strong>Random-intercept model</strong>: each group receives a <strong>different intercept</strong> in addition to the global intercept.</p> <li><p><strong>Random-slope model</strong>: each group receives <strong>different coefficients</strong> for each &#40;or a subset of&#41; independent variable&#40;s&#41; in addition to a global intercept.</p> <li><p><strong>Random-intercept-slope model</strong>: each group receives <strong>both a different intercept and different coefficients</strong> for each independent variable in addition to a global intercept.</p> </ol> <h3 id=random-intercept_model ><a href="#random-intercept_model" class=header-anchor >Random-Intercept Model</a></h3> <p>The first approach is the <strong>random-intercept model</strong> in which we specify a different intercept for each group, in addition to the global intercept. These group-level intercepts are sampled from a hyperprior.</p> <p>To illustrate a multilevel model, I will use the linear regression example with a Gaussian/normal likelihood function. Mathematically a Bayesian multilevel random-slope linear regression model is:</p> \[ \begin{aligned} \mathbf{y} &\sim \text{Normal}\left( \alpha + \alpha_j + \mathbf{X} \cdot \boldsymbol{\beta}, \sigma \right) \\ \alpha &\sim \text{Normal}(\mu_\alpha, \sigma_\alpha) \\ \alpha_j &\sim \text{Normal}(0, \tau) \\ \boldsymbol{\beta} &\sim \text{Normal}(\mu_{\boldsymbol{\beta}}, \sigma_{\boldsymbol{\beta}}) \\ \tau &\sim \text{Cauchy}^+(0, \psi_{\alpha})\\ \sigma &\sim \text{Exponential}(\lambda_\sigma) \end{aligned} \] <p>The priors on the global intercept \(\alpha\), global coefficients \(\boldsymbol{\beta}\) and error \(\sigma\), along with the Gaussian/normal likelihood on \(\mathbf{y}\) are the same as in the linear regression model. But now we have <strong>new parameters</strong>. The first are the <strong>group intercepts</strong> prior \(\alpha_j\) that denotes that every group \(1, 2, \dots, J\) has its own intercept sampled from a normal distribution centered on 0 with a standard deviation \(\psi_\alpha\). This group intercept is added to the linear predictor inside the Gaussian/normal likelihood function. The <strong>group intercepts&#39; standard deviation</strong> \(\tau\) have a hyperprior &#40;being a prior of a prior&#41; which is sampled from a positive-constrained Cauchy distribution &#40;a special case of the Student-\(t\) distribution with degrees of freedom \(\nu = 1\)&#41; with mean 0 and standard deviation \(\sigma_\alpha\). This makes the group-level intercept&#39;s dispersions being sampled from the same parameter \(\tau\) which allows the model to use information from one group intercept to infer robust information regarding another group&#39;s intercept dispersion and so on.</p> <p>This is easily accomplished with Turing:</p> <pre><code class=language-julia >using Turing
using Statistics: mean, std
using Random:seed&#33;
seed&#33;&#40;123&#41;

@model varying_intercept&#40;X, idx, y; n_gr&#61;length&#40;unique&#40;idx&#41;&#41;, predictors&#61;size&#40;X, 2&#41;&#41; &#61; begin
    #priors
    α ~ Normal&#40;mean&#40;y&#41;, 2.5 * std&#40;y&#41;&#41;       # population-level intercept
    β ~ filldist&#40;Normal&#40;0, 2&#41;, predictors&#41;  # population-level coefficients
    σ ~ Exponential&#40;1 / std&#40;y&#41;&#41;             # residual SD
    #prior for variance of random intercepts
    #usually requires thoughtful specification
    τ ~ truncated&#40;Cauchy&#40;0, 2&#41;, 0, Inf&#41;     # group-level SDs intercepts
    αⱼ ~ filldist&#40;Normal&#40;0, τ&#41;, n_gr&#41;       # group-level intercepts

    #likelihood
    ŷ &#61; α .&#43; X * β .&#43; αⱼ&#91;idx&#93;
    y ~ MvNormal&#40;ŷ, σ&#41;
end;</code></pre> <h3 id=random-slope_model ><a href="#random-slope_model" class=header-anchor >Random-Slope Model</a></h3> <p>The second approach is the <strong>random-slope model</strong> in which we specify a different slope for each group, in addition to the global intercept. These group-level slopes are sampled from a hyperprior.</p> <p>To illustrate a multilevel model, I will use the linear regression example with a Gaussian/normal likelihood function. Mathematically a Bayesian multilevel random-slope linear regression model is:</p> \[ \begin{aligned} \mathbf{y} &\sim \text{Normal}\left( \alpha + \mathbf{X} \cdot \boldsymbol{\beta}_j \cdot \boldsymbol{\tau}, \sigma \right) \\ \alpha &\sim \text{Normal}(\mu_\alpha, \sigma_\alpha) \\ \boldsymbol{\beta}_j &\sim \text{Normal}(0, 1) \\ \boldsymbol{\tau} &\sim \text{Cauchy}^+(0, \psi_{\boldsymbol{\beta}})\\ \sigma &\sim \text{Exponential}(\lambda_\sigma) \end{aligned} \] <p>Here we have a similar situation from before with the same hyperprior, but now it is a hyperprior for the the group coefficients&#39; standard deviation prior: \(\boldsymbol{\beta}_j\). This makes the group-level coefficients&#39;s dispersions being sampled from the same parameter \(\tau\) which allows the model to use information from one group coefficients to infer robust information regarding another group&#39;s coefficients dispersion and so on.</p> <p>In Turing we can accomplish this as:</p> <pre><code class=language-julia >@model varying_slope&#40;X, idx, y; n_gr&#61;length&#40;unique&#40;idx&#41;&#41;, predictors&#61;size&#40;X, 2&#41;&#41; &#61; begin
    #priors
    α ~ Normal&#40;mean&#40;y&#41;, 2.5 * std&#40;y&#41;&#41;                   # population-level intercept
    σ ~ Exponential&#40;1 / std&#40;y&#41;&#41;                         # residual SD
    #prior for variance of random slopes
    #usually requires thoughtful specification
    τ ~ filldist&#40;truncated&#40;Cauchy&#40;0, 2&#41;, 0, Inf&#41;, n_gr&#41; # group-level slopes SDs
    βⱼ ~ filldist&#40;Normal&#40;0, 1&#41;, predictors, n_gr&#41;       # group-level standard normal slopes

    #likelihood
    ŷ &#61; α .&#43; X * βⱼ * τ
    y ~ MvNormal&#40;ŷ, σ&#41;
end;</code></pre> <h3 id=random-intercept-slope_model ><a href="#random-intercept-slope_model" class=header-anchor >Random-Intercept-Slope Model</a></h3> <p>The third approach is the <strong>random-intercept-slope model</strong> in which we specify a different intercept and slope for each group, in addition to the global intercept. These group-level intercepts and slopes are sampled from hyperpriors.</p> <p>To illustrate a multilevel model, I will use the linear regression example with a Gaussian/normal likelihood function. Mathematically a Bayesian multilevel random-intercept-slope linear regression model is:</p> \[ \begin{aligned} \mathbf{y} &\sim \text{Normal}\left( \alpha + \alpha_j + \mathbf{X} \cdot \boldsymbol{\beta}_j \cdot \boldsymbol{\tau}_{\boldsymbol{\beta}}, \sigma \right) \\ \alpha &\sim \text{Normal}(\mu_\alpha, \sigma_\alpha) \\ \alpha_j &\sim \text{Normal}(0, \tau_{\alpha}) \\ \boldsymbol{\beta}_j &\sim \text{Normal}(0, 1) \\ \tau_{\alpha} &\sim \text{Cauchy}^+(0, \psi_{\alpha})\\ \boldsymbol{\tau}_{\boldsymbol{\beta}} &\sim \text{Cauchy}^+(0, \psi_{\boldsymbol{\beta}})\\ \sigma &\sim \text{Exponential}(\lambda_\sigma) \end{aligned} \] <p>Here we have a similar situation from before with the same hyperpriors, but now we fused both random-intercept and random-slope together.</p> <p>In Turing we can accomplish this as:</p> <pre><code class=language-julia >@model varying_intercept_slope&#40;X, idx, y; n_gr&#61;length&#40;unique&#40;idx&#41;&#41;, predictors&#61;size&#40;X, 2&#41;&#41; &#61; begin
    #priors
    α ~ Normal&#40;mean&#40;y&#41;, 2.5 * std&#40;y&#41;&#41;                    # population-level intercept
    σ ~ Exponential&#40;1 / std&#40;y&#41;&#41;                          # residual SD
    #prior for variance of random intercepts and slopes
    #usually requires thoughtful specification
    τₐ ~ truncated&#40;Cauchy&#40;0, 2&#41;, 0, Inf&#41;                 # group-level SDs intercepts
    τᵦ ~ filldist&#40;truncated&#40;Cauchy&#40;0, 2&#41;, 0, Inf&#41;, n_gr&#41; # group-level slopes SDs
    αⱼ ~ filldist&#40;Normal&#40;0, τₐ&#41;, n_gr&#41;                   # group-level intercepts
    βⱼ ~ filldist&#40;Normal&#40;0, 1&#41;, predictors, n_gr&#41;        # group-level standard normal slopes

    #likelihood
    ŷ &#61; α .&#43; αⱼ&#91;idx&#93; .&#43; X * βⱼ * τᵦ
    y ~ MvNormal&#40;ŷ, σ&#41;
end;</code></pre> <h2 id=example_-_cheese_ratings ><a href="#example_-_cheese_ratings" class=header-anchor >Example - Cheese Ratings</a></h2> <p>For our example, I will use a famous dataset called <code>cheese</code> &#40;Boatwright, McCulloch &amp; Rossi, 1999&#41;, which is data from cheese ratings. A group of 10 rural and 10 urban raters rated 4 types of different cheeses &#40;A, B, C and D&#41; in two samples. So we have \(4 \cdot 20 \cdot2 = 160\) observations and 4 variables:</p> <ul> <li><p><code>cheese</code>: type of cheese from <code>A</code> to <code>D</code></p> <li><p><code>rater</code>: id of the rater from <code>1</code> to <code>10</code></p> <li><p><code>background</code>: type of rater, either <code>rural</code> or <code>urban</code></p> <li><p><code>y</code>: rating of the cheese</p> </ul> <p>Ok let&#39;s read our data with <code>CSV.jl</code> and output into a <code>DataFrame</code> from <code>DataFrames.jl</code>:</p> <pre><code class=language-julia >using DataFrames, CSV, HTTP

url &#61; &quot;https://raw.githubusercontent.com/storopoli/Bayesian-Julia/master/datasets/cheese.csv&quot;
cheese &#61; CSV.read&#40;HTTP.get&#40;url&#41;.body, DataFrame&#41;
describe&#40;cheese&#41;</code></pre><pre><code class="plaintext code-output">4×7 DataFrame
 Row │ variable    mean     min    median  max    nmissing  eltype
     │ Symbol      Union…   Any    Union…  Any    Int64     DataType
─────┼───────────────────────────────────────────────────────────────
   1 │ cheese               A              D             0  String1
   2 │ rater       5.5      1      5.5     10            0  Int64
   3 │ background           rural          urban         0  String7
   4 │ y           70.8438  33     71.5    91            0  Int64</code></pre> <p>As you can see from the <code>describe&#40;&#41;</code> output, the mean cheese ratings is around 70 ranging from 33 to 91.</p> <p>In order to prepare the data for Turing, I will convert the <code>String</code>s in variables <code>cheese</code> and <code>background</code> to <code>Int</code>s. Regarding <code>cheese</code>, I will create 4 dummy variables one for each cheese type; and <code>background</code> will be converted to integer data taking two values: one for each background type. My intent is to model <code>background</code> as a group both for intercept and coefficients. Take a look at how the data will look like for the first 5 observations:</p> <pre><code class=language-julia >for c in unique&#40;cheese&#91;:, :cheese&#93;&#41;
    cheese&#91;:, &quot;cheese_&#36;c&quot;&#93; &#61; ifelse.&#40;cheese&#91;:, :cheese&#93; .&#61;&#61; c, 1, 0&#41;
end

cheese&#91;:, :background_int&#93; &#61; map&#40;cheese&#91;:, :background&#93;&#41; do b
    b &#61;&#61; &quot;rural&quot; ? 1 :
    b &#61;&#61; &quot;urban&quot; ? 2 : missing
end

first&#40;cheese, 5&#41;</code></pre><pre><code class="plaintext code-output">5×9 DataFrame
 Row │ cheese    rater  background  y      cheese_A  cheese_B  cheese_C  cheese_D  background_int
     │ String1…  Int64  String7…    Int64  Int64     Int64     Int64     Int64     Int64
─────┼────────────────────────────────────────────────────────────────────────────────────────────
   1 │ A             1  rural          67         1         0         0         0               1
   2 │ A             1  rural          66         1         0         0         0               1
   3 │ B             1  rural          51         0         1         0         0               1
   4 │ B             1  rural          53         0         1         0         0               1
   5 │ C             1  rural          75         0         0         1         0               1</code></pre> <p>Now let&#39;s us instantiate our model with the data. Here, I will specify a vector of <code>Int</code>s named <code>idx</code> to represent the different observations&#39; group memberships. This will be used by Turing when we index a parameter with the <code>idx</code>, <em>e.g.</em> <code>αⱼ&#91;idx&#93;</code>.</p> <pre><code class=language-julia >X &#61; Matrix&#40;select&#40;cheese, Between&#40;:cheese_A, :cheese_D&#41;&#41;&#41;;
y &#61; cheese&#91;:, :y&#93;;
idx &#61; cheese&#91;:, :background_int&#93;;</code></pre> <p>The first model is the <code>varying_intercept</code>:</p> <pre><code class=language-julia >model_intercept &#61; varying_intercept&#40;X, idx, y&#41;
chain_intercept &#61; sample&#40;model_intercept, NUTS&#40;&#41;, MCMCThreads&#40;&#41;, 2_000, 4&#41;
summarystats&#40;chain_intercept&#41;  |&gt; DataFrame  |&gt; println</code></pre><pre><code class="plaintext code-output">9×8 DataFrame
 Row │ parameters  mean       std       naive_se    mcse        ess      rhat     ess_per_sec
     │ Symbol      Float64    Float64   Float64     Float64     Float64  Float64  Float64
─────┼────────────────────────────────────────────────────────────────────────────────────────
   1 │ α            70.9202   5.33286   0.0596232   0.127038    1557.59  0.99995      32.6587
   2 │ β[1]          3.23581  1.25415   0.0140218   0.023304    3224.24  1.00135      67.604
   3 │ β[2]        -11.6113   1.24938   0.0139685   0.0228865   2929.34  1.00231      61.4208
   4 │ β[3]          7.20121  1.26355   0.0141269   0.0228724   3419.05  1.00111      71.6888
   5 │ β[4]          1.23636  1.25291   0.0140079   0.0202693   3546.94  1.00106      74.3703
   6 │ σ             5.99912  0.269684  0.00301516  0.00323454  5381.43  0.99992     112.835
   7 │ τ             6.60777  6.68737   0.0747671   0.148843    1579.61  1.00167      33.1204
   8 │ αⱼ[1]        -3.67621  5.22431   0.0584096   0.125214    1520.63  1.00025      31.8836
   9 │ αⱼ[2]         3.49673  5.21951   0.0583559   0.125978    1529.61  1.00016      32.072
</code></pre> <p>Here we can see that the model has a population-level intercept <code>α</code> along with population-level coefficients <code>β</code>s for each <code>cheese</code> dummy variable. But notice that we have also group-level intercepts for each of the groups <code>αⱼ</code>s. Specifically, <code>αⱼ&#91;1&#93;</code> are the rural raters and <code>αⱼ&#91;2&#93;</code> are the urban raters.</p> <p>Now let&#39;s go to the second model, <code>varying_slope</code>:</p> <pre><code class=language-julia >model_slope &#61; varying_slope&#40;X, idx, y&#41;
chain_slope &#61; sample&#40;model_slope, NUTS&#40;&#41;, MCMCThreads&#40;&#41;, 2_000, 4&#41;
summarystats&#40;chain_slope&#41;  |&gt; DataFrame  |&gt; println</code></pre><pre><code class="plaintext code-output">12×8 DataFrame
 Row │ parameters  mean        std       naive_se    mcse        ess      rhat     ess_per_sec
     │ Symbol      Float64     Float64   Float64     Float64     Float64  Float64  Float64
─────┼─────────────────────────────────────────────────────────────────────────────────────────
   1 │ α           70.8114     4.79924   0.0536571   0.101646    2101.85  1.00087     16.1914
   2 │ σ            6.54192    0.285938  0.00319688  0.00416645  5383.29  1.00037     41.4696
   3 │ τ[1]         6.05703    4.91366   0.0549364   0.137627    1281.8   1.00115      9.87419
   4 │ τ[2]         6.05027    5.00599   0.0559686   0.147636    1153.73  1.00123      8.88761
   5 │ βⱼ[1,1]      0.243822   0.811422  0.00907197  0.0156275   3187.9   1.0015      24.5577
   6 │ βⱼ[2,1]     -0.927608   1.03722   0.0115965   0.0264846   1644.87  1.0001      12.671
   7 │ βⱼ[3,1]      0.562864   0.877836  0.0098145   0.0199541   2372.51  1.0006      18.2764
   8 │ βⱼ[4,1]      0.0969922  0.799195  0.00893528  0.0136793   3056.19  1.00085     23.5431
   9 │ βⱼ[1,2]      0.258359   0.807021  0.00902277  0.0156428   2795.54  1.00193     21.5351
  10 │ βⱼ[2,2]     -0.917479   1.05359   0.0117794   0.024764    1797.64  1.00027     13.848
  11 │ βⱼ[3,2]      0.567299   0.902078  0.0100855   0.0175936   2502.98  1.0006      19.2814
  12 │ βⱼ[4,2]      0.107948   0.797702  0.00891858  0.0132952   3685.84  1.00135     28.3935
</code></pre> <p>Here we can see that the model has still a population-level intercept <code>α</code>. But now our population-level coefficients <code>β</code>s are replaced by group-level coefficients <code>βⱼ</code>s along with their standard deviation <code>τᵦ</code>s. Specifically <code>βⱼ</code>&#39;s first index denotes the 4 dummy <code>cheese</code> variables&#39; and the second index are the group membership. So, for example <code>βⱼ&#91;1,1&#93;</code> is the coefficient for <code>cheese_A</code> and rural raters &#40;group 1&#41;.</p> <p>Now let&#39;s go to the third model, <code>varying_intercept_slope</code>:</p> <pre><code class=language-julia >model_intercept_slope &#61; varying_intercept_slope&#40;X, idx, y&#41;
chain_intercept_slope &#61; sample&#40;model_intercept_slope, NUTS&#40;&#41;, MCMCThreads&#40;&#41;, 2_000, 4&#41;
summarystats&#40;chain_intercept_slope&#41;  |&gt; DataFrame  |&gt; println</code></pre><pre><code class="plaintext code-output">15×8 DataFrame
 Row │ parameters  mean        std       naive_se    mcse        ess       rhat      ess_per_sec
     │ Symbol      Float64     Float64   Float64     Float64     Float64   Float64   Float64
─────┼───────────────────────────────────────────────────────────────────────────────────────────
   1 │ α           71.0626     7.83209   0.0875655   0.240281    1103.11   1.00238       3.81383
   2 │ σ            5.87618    0.265073  0.00296361  0.00345513  7032.87   1.00004      24.3151
   3 │ τₐ           6.70437    7.51895   0.0840645   0.224019    1372.05   1.0018        4.74365
   4 │ τᵦ[1]        6.36806    5.68283   0.063536    0.135849    1508.82   1.00161       5.2165
   5 │ τᵦ[2]        6.37695    5.73307   0.0640977   0.176432     963.377  1.00279       3.33073
   6 │ αⱼ[1]       -3.89769    5.94114   0.066424    0.219604     799.028  1.0036        2.76252
   7 │ αⱼ[2]        3.28275    5.92363   0.0662282   0.216349     811.2    1.00343       2.8046
   8 │ βⱼ[1,1]      0.246034   0.831311  0.00929434  0.0176387   2208.44   1.0013        7.63535
   9 │ βⱼ[2,1]     -0.888226   1.05528   0.0117984   0.0231345   2157.27   1.00124       7.45843
  10 │ βⱼ[3,1]      0.556325   0.921074  0.0102979   0.0189355   2003.7    1.00009       6.92747
  11 │ βⱼ[4,1]      0.0953832  0.779033  0.00870985  0.013333    3880.57   1.00038      13.4165
  12 │ βⱼ[1,2]      0.264749   0.806497  0.00901691  0.0122747   3961.52   1.00036      13.6964
  13 │ βⱼ[2,2]     -0.889554   1.0405    0.0116331   0.0203335   2363.47   1.00235       8.17133
  14 │ βⱼ[3,2]      0.550742   0.897587  0.0100353   0.0174808   2898.1    1.0006       10.0198
  15 │ βⱼ[4,2]      0.103328   0.791032  0.00884401  0.0115532   4253.39   0.999846     14.7054
</code></pre> <p>Now we have fused the previous model in one. We still have a population-level intercept <code>α</code>. But now we have in the same model group-level intercepts for each of the groups <code>αⱼ</code>s and group-level along with their standard deviation <code>τₐ</code>. We also have the coefficients <code>βⱼ</code>s with their standard deviation <code>τᵦ</code>s. The parameters are interpreted exactly as the previous cases.</p> <h2 id=references ><a href="#references" class=header-anchor >References</a></h2> <p>Boatwright, P., McCulloch, R., &amp; Rossi, P. &#40;1999&#41;. Account-level modeling for trade promotion: An application of a constrained parameter hierarchical model. Journal of the American Statistical Association, 94&#40;448&#41;, 1063–1073.</p> <p>de Finetti, B. &#40;1974&#41;. Theory of Probability &#40;Volume 1&#41;. New York: John Wiley &amp; Sons.</p> <p>Nau, R. F. &#40;2001&#41;. De Finetti was Right: Probability Does Not Exist. Theory and Decision, 51&#40;2&#41;, 89–124. https://doi.org/10.1023/A:1015525808214</p> <div class=page-foot > <div class=copyright > Last modified: November 09, 2021. Website built with <a href="https://github.com/tlienart/Franklin.jl">Franklin.jl</a> and the <a href="https://julialang.org">Julia programming language</a>. </div> </div> </div> </div> </div> </div> <!-- end of class page-wrap--> <script src="/Bayesian-Julia/libs/katex/katex.min.js"></script> <script src="/Bayesian-Julia/libs/katex/auto-render.min.js"></script> <script>renderMathInElement(document.body)</script> <script src="/Bayesian-Julia/libs/highlight/highlight.pack.js"></script> <script>hljs.initHighlightingOnLoad();hljs.configure({tabReplace: ' '});</script>