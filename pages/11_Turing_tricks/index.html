<!doctype html> <html lang=en > <meta charset=UTF-8 > <meta name=viewport  content="width=device-width, initial-scale=1"> <link rel=stylesheet  href="/Bayesian-Julia/libs/katex/katex.min.css"> <link rel=stylesheet  href="/Bayesian-Julia/libs/highlight/github.min.css"> <link rel=stylesheet  href="/Bayesian-Julia/css/jtd.css"> <link rel=icon  href="/Bayesian-Julia/assets/favicon.ico"> <title>Computational Tricks with Turing <br/> &#40;Non-Centered Parametrization <br/> and QR Decomposition&#41;</title> <div class=page-wrap > <div class=side-bar > <div class=header > <a href="/Bayesian-Julia/" class=title > Bayesian Stats </a> </div> <label for=show-menu  class=show-menu >MENU</label> <input type=checkbox  id=show-menu  role=button > <div class=menu  id=side-menu > <ul class=menu-list > <li class="menu-list-item "><a href="/Bayesian-Julia/" class="menu-list-link ">Home</a> <li class="menu-list-item "><a href="/Bayesian-Julia/pages/1_why_Julia/" class="menu-list-link ">1. Why Julia?</a> <li class="menu-list-item "><a href="/Bayesian-Julia/pages/2_bayes_stats/" class="menu-list-link ">2. What is Bayesian Statistics?</a> <li class="menu-list-item "><a href="/Bayesian-Julia/pages/3_prob_dist/" class="menu-list-link ">3. Common Probability Distributions</a> <li class="menu-list-item "><a href="/Bayesian-Julia/pages/4_Turing/" class="menu-list-link ">4. How to use Turing</a> <li class="menu-list-item "><a href="/Bayesian-Julia/pages/5_MCMC/" class="menu-list-link ">5. Markov Chain Monte Carlo (MCMC)</a> <li class="menu-list-item "><a href="/Bayesian-Julia/pages/6_linear_reg/" class="menu-list-link ">6. Bayesian Linear Regression</a> <li class="menu-list-item "><a href="/Bayesian-Julia/pages/7_logistic_reg/" class="menu-list-link ">7. Bayesian Logistic Regression</a> <li class="menu-list-item "><a href="/Bayesian-Julia/pages/8_count_reg/" class="menu-list-link ">8. Bayesian Regression with Count Data</a> <li class="menu-list-item "><a href="/Bayesian-Julia/pages/9_robust_reg/" class="menu-list-link ">9. Robust Bayesian Regression</a> <li class="menu-list-item "><a href="/Bayesian-Julia/pages/10_multilevel_models/" class="menu-list-link ">10. Multilevel Models</a> <li class="menu-list-item active"><a href="/Bayesian-Julia/pages/11_Turing_tricks/" class="menu-list-link active">11. Computational Tricks with Turing</a> <li class="menu-list-item "><a href="/Bayesian-Julia/pages/12_epi_models/" class="menu-list-link ">12. Bayesian Epidemiological Models</a> </ul> </div> <div class=footer > <a href="https://www.julialang.org"><img style="height:50px;padding-left:10px;margin-bottom:15px;" src="https://julialang.org/assets/infra/logo.svg" alt="Julia Logo"></a> </div> </div> <div class=main-content-wrap > <div class=main-content > <div class=main-header > <a id=github  href="https://github.com/storopoli/Bayesian-Julia">Code on GitHub</a> </div> <script async src="https://www.googletagmanager.com/gtag/js?id=UA-186284914-6"></script> <script> window.dataLayer = window.dataLayer || []; function gtag(){dataLayer.push(arguments);} gtag('js', new Date()); gtag('config', 'UA-186284914-6'); </script> <div class=franklin-content ><div class=franklin-toc ><ol><li><a href="#qr_decomposition">QR Decomposition</a><li><a href="#non-centered_parametrization">Non-Centered Parametrization</a><li><a href="#references">References</a></ol></div> <h1 id=computational_tricks_with_turing_non-centered_parametrization_and_qr_decomposition ><a href="#computational_tricks_with_turing_non-centered_parametrization_and_qr_decomposition" class=header-anchor >Computational Tricks with Turing <br/> &#40;Non-Centered Parametrization <br/> and QR Decomposition&#41;</a></h1> <p>There are some computational tricks that we can employ with Turing. I will cover here two computational tricks:</p> <ol> <li><p><strong>QR Decomposition</strong></p> <li><p><strong>Non-Centered Parametrization</strong></p> </ol> <h2 id=qr_decomposition ><a href="#qr_decomposition" class=header-anchor >QR Decomposition</a></h2> <p>Back in &quot;Linear Algebra 101&quot; we&#39;ve learned that any matrix &#40;even rectangular ones&#41; can be factored into the product of two matrices:</p> <ul> <li><p>\(\mathbf{Q}\): an orthogonal matrix &#40;its columns are orthogonal unit vectors meaning \(\mathbf{Q}^T = \mathbf{Q}^{-1})\).</p> <li><p>\(\mathbf{R}\): an upper triangular matrix.</p> </ul> <p>This is commonly known as the <a href="https://en.wikipedia.org/wiki/QR_decomposition"><strong>QR Decomposition</strong></a>:</p> \[ \mathbf{A} = \mathbf{Q} \cdot \mathbf{R} \] <p>Let me show you an example with a random matrix \(\mathbf{A} \in \mathbb{R}^{3 \times 2}\):</p> <pre><code class=language-julia >A &#61; rand&#40;3, 2&#41;</code></pre><pre><code class="plaintext code-output">3×2 Matrix{Float64}:
 0.420689  0.0462698
 0.103548  0.302975
 0.212115  0.415854</code></pre> <p>Now let&#39;s factor <code>A</code> using <code>LinearAlgebra</code>&#39;s <code>qr&#40;&#41;</code> function:</p> <pre><code class=language-julia >using LinearAlgebra:qr, I
Q, R &#61; qr&#40;A&#41;</code></pre><pre><code class="plaintext code-output">LinearAlgebra.QRCompactWY{Float64, Matrix{Float64}}
Q factor:
3×3 LinearAlgebra.QRCompactWYQ{Float64, Matrix{Float64}}:
 -0.872104   0.478455  -0.102539
 -0.214658  -0.562401  -0.798516
 -0.439722  -0.674378   0.593177
R factor:
2×2 Matrix{Float64}:
 -0.482384  -0.288248
  0.0       -0.428698</code></pre> <p>Notice that <code>qr&#40;&#41;</code> produced a tuple containing two matrices <code>Q</code> and <code>R</code>. <code>Q</code> is a 3x3 orthogonal matrix. And <code>R</code> is a 2x2 upper triangular matrix. So that \(\mathbf{Q}^T = \mathbf{Q}^{-1}\) &#40;the transpose is equal the inverse&#41;:</p> <pre><code class=language-julia >Matrix&#40;Q&#39;&#41; ≈ Matrix&#40;Q^-1&#41;</code></pre><pre><code class="plaintext code-output">true</code></pre>
<p>Also note that \(\mathbf{Q}^T \cdot \mathbf{Q}^{-1} = \mathbf{I}\) &#40;identity matrix&#41;:</p>
<pre><code class=language-julia >Q&#39; * Q ≈ I&#40;3&#41;</code></pre><pre><code class="plaintext code-output">true</code></pre>
<p>This is nice. But what can we do with QR decomposition? It can speed up Turing&#39;s sampling by a huge factor while also decorrelating the columns of \(\mathbf{X}\), <em>i.e.</em> the independent variables. The orthogonal nature of QR decomposition alters the posterior&#39;s topology and makes it easier for HMC or other MCMC samplers to explore it. Let&#39;s see how fast we can get with QR decomposition. First, let&#39;s go back to the <code>kidiq</code> example in <a href="/Bayesian-Julia/pages/6_linear_reg/">6. <strong>Bayesian Linear Regression</strong></a>:</p>
<pre><code class=language-julia >using Turing
using Statistics: mean, std
using Random:seed&#33;
seed&#33;&#40;123&#41;

@model linreg&#40;X, y; predictors&#61;size&#40;X, 2&#41;&#41; &#61; begin
    #priors
    α ~ Normal&#40;mean&#40;y&#41;, 2.5 * std&#40;y&#41;&#41;
    β ~ filldist&#40;TDist&#40;3&#41;, predictors&#41;
    σ ~ Exponential&#40;1&#41;

    #likelihood
    y ~ MvNormal&#40;α .&#43; X * β, σ&#41;
end;

using DataFrames, CSV, HTTP

url &#61; &quot;https://raw.githubusercontent.com/storopoli/Bayesian-Julia/master/datasets/kidiq.csv&quot;
kidiq &#61; CSV.read&#40;HTTP.get&#40;url&#41;.body, DataFrame&#41;
X &#61; Matrix&#40;select&#40;kidiq, Not&#40;:kid_score&#41;&#41;&#41;
y &#61; kidiq&#91;:, :kid_score&#93;
model &#61; linreg&#40;X, y&#41;
chain &#61; sample&#40;model, NUTS&#40;&#41;, MCMCThreads&#40;&#41;, 2_000, 4&#41;</code></pre><pre><code class="plaintext code-output">Chains MCMC chain (2000×17×4 Array{Float64, 3}):

Iterations        = 1001:1:3000
Number of chains  = 4
Samples per chain = 2000
Wall duration     = 28.01 seconds
Compute duration  = 53.82 seconds
parameters        = α, β[1], β[2], β[3], σ
internals         = lp, n_steps, is_accept, acceptance_rate, log_density, hamiltonian_energy, hamiltonian_energy_error, max_hamiltonian_energy_error, tree_depth, numerical_error, step_size, nom_step_size

Summary Statistics
  parameters      mean       std   naive_se      mcse         ess      rhat   ess_per_sec
      Symbol   Float64   Float64    Float64   Float64     Float64   Float64       Float64

           α   21.5413    8.7244     0.0975    0.1594   3063.8728    1.0024       56.9239
        β[1]    2.0539    1.8404     0.0206    0.0289   3784.0678    1.0001       70.3045
        β[2]    0.5789    0.0596     0.0007    0.0010   3673.2601    1.0008       68.2458
        β[3]    0.2533    0.3031     0.0034    0.0051   3844.6548    1.0023       71.4301
           σ   17.8888    0.5854     0.0065    0.0075   5353.3577    1.0001       99.4604

Quantiles
  parameters      2.5%     25.0%     50.0%     75.0%     97.5%
      Symbol   Float64   Float64   Float64   Float64   Float64

           α    3.6406   15.7614   21.6255   27.4491   38.4661
        β[1]   -0.6528    0.7038    1.7204    3.1036    6.3133
        β[2]    0.4604    0.5387    0.5786    0.6197    0.6946
        β[3]   -0.3131    0.0433    0.2494    0.4559    0.8637
           σ   16.8012   17.4908   17.8723   18.2690   19.1017
</code></pre>
<p>See the wall duration in Turing&#39;s <code>chain</code>: for me it took around 24 seconds.</p>
<p>Now let&#39;s us incorporate QR decomposition in the linear regression model. Here, I will use the &quot;thin&quot; instead of the &quot;fat&quot; QR, which scales the \(\mathbf{Q}\) and \(\mathbf{R}\) matrices by a factor of \(\sqrt{n-1}\) where \(n\) is the number of rows of \(\mathbf{X}\). In practice it is better to implement the thin QR decomposition, which is to be preferred to the fat QR decomposition. It is numerically more stable. Mathematically, the thin QR decomposition is:</p>
\[
\begin{aligned}
x &= \mathbf{Q}^* \mathbf{R}^* \\
\mathbf{Q}^* &= \mathbf{Q} \cdot \sqrt{n - 1} \\
\mathbf{R}^* &= \frac{1}{\sqrt{n - 1}} \cdot \mathbf{R}\\
\boldsymbol{\mu}
&= \alpha + \mathbf{X} \cdot \boldsymbol{\beta} + \sigma
\\
&= \alpha + \mathbf{Q}^* \cdot \mathbf{R}^* \cdot \boldsymbol{\beta} + \sigma
\\
&= \alpha + \mathbf{Q}^* \cdot (\mathbf{R}^* \cdot \boldsymbol{\beta}) + \sigma
\\
&= \alpha + \mathbf{Q}^* \cdot \widetilde{\boldsymbol{\beta}} + \sigma
\\
\end{aligned}
\]
<p>Then we can recover the original \(\boldsymbol{\beta}\) with:</p>
\[ \boldsymbol{\beta} = \mathbf{R}^{*-1} \cdot \widetilde{\boldsymbol{\beta}} \]
<p>In Turing, a model with QR decomposition would be the same <code>linreg</code> but with a different <code>X</code> matrix supplied, since it is a data transformation. First, we decompose your model data <code>X</code> into <code>Q</code> and <code>R</code>:</p>
<pre><code class=language-julia >Q, R &#61; qr&#40;X&#41;
Q_ast &#61; Matrix&#40;Q&#41; * sqrt&#40;size&#40;X, 1&#41; - 1&#41;
R_ast &#61; R / sqrt&#40;size&#40;X, 1&#41; - 1&#41;;</code></pre>
<p>Then, we instantiate a model with <code>Q</code> instead of <code>X</code> and sample as you would:</p>
<pre><code class=language-julia >model_qr &#61; linreg&#40;Q_ast, y&#41;
chain_qr &#61; sample&#40;model_qr, NUTS&#40;1_000, 0.65&#41;, MCMCThreads&#40;&#41;, 2_000, 4&#41;</code></pre><pre><code class="plaintext code-output">Chains MCMC chain (2000×17×4 Array{Float64, 3}):

Iterations        = 1001:1:3000
Number of chains  = 4
Samples per chain = 2000
Wall duration     = 10.66 seconds
Compute duration  = 20.94 seconds
parameters        = α, β[1], β[2], β[3], σ
internals         = lp, n_steps, is_accept, acceptance_rate, log_density, hamiltonian_energy, hamiltonian_energy_error, max_hamiltonian_energy_error, tree_depth, numerical_error, step_size, nom_step_size

Summary Statistics
  parameters       mean       std   naive_se      mcse         ess      rhat   ess_per_sec
      Symbol    Float64   Float64    Float64   Float64     Float64   Float64       Float64

           α    33.3363    7.9131     0.0885    0.1652   1971.0449    1.0004       94.1417
        β[1]   -49.6184    7.0957     0.0793    0.1474   1978.9367    1.0005       94.5186
        β[2]    21.8851    3.6004     0.0403    0.0743   2021.3357    1.0005       96.5437
        β[3]     0.3135    0.8927     0.0100    0.0151   2681.5544    1.0006      128.0773
           σ    17.8778    0.5922     0.0066    0.0084   4909.9719    0.9997      234.5117

Quantiles
  parameters       2.5%      25.0%      50.0%      75.0%      97.5%
      Symbol    Float64    Float64    Float64    Float64    Float64

           α    18.1713    28.0534    33.1041    38.4377    49.1678
        β[1]   -63.3144   -54.2649   -49.7822   -45.0278   -35.3723
        β[2]    14.6466    19.5234    21.9702    24.2787    28.7963
        β[3]    -1.3902    -0.2668     0.2736     0.8354     2.2015
           σ    16.7538    17.4745    17.8695    18.2621    19.0818
</code></pre>
<p>See the wall duration in Turing&#39;s <code>chain_qr</code>: for me it took around 5 seconds. Much faster than the regular <code>linreg</code>. Now we have to reconstruct our \(\boldsymbol{\beta}\)s:</p>
<pre><code class=language-julia >betas &#61; mapslices&#40;x -&gt; R_ast^-1 * x, chain_qr&#91;:, namesingroup&#40;chain_qr, :β&#41;,:&#93;.value.data, dims&#61;&#91;2&#93;&#41;
chain_qr_reconstructed &#61; hcat&#40;Chains&#40;betas, &#91;&quot;real_β&#91;&#36;i&#93;&quot; for i in 1:size&#40;Q_ast, 2&#41;&#93;&#41;, chain_qr&#41;</code></pre><pre><code class="plaintext code-output">ArgumentError: chain ranges differ
</code></pre>
<h2 id=non-centered_parametrization ><a href="#non-centered_parametrization" class=header-anchor >Non-Centered Parametrization</a></h2>
<p>Now let&#39;s us explore <strong>Non-Centered Parametrization</strong> &#40;NCP&#41;. This is useful when the posterior&#39;s topology is very difficult to explore as has regions where HMC sampler has to change the step size \(L\) and the \(\epsilon\) factor. This is  I&#39;ve showed one of the most infamous case in <a href="/Bayesian-Julia/pages/5_MCMC/">5. <strong>Markov Chain Monte Carlo &#40;MCMC&#41;</strong></a>: Neal&#39;s Funnel &#40;Neal, 2003&#41;:</p>
<pre><code class=language-julia >using StatsPlots, Distributions, LaTeXStrings
funnel_y &#61; rand&#40;Normal&#40;0, 3&#41;, 10_000&#41;
funnel_x &#61; rand&#40;Normal&#40;&#41;, 10_000&#41; .* exp.&#40;funnel_y / 2&#41;

scatter&#40;&#40;funnel_x, funnel_y&#41;,
        label&#61;false, mc&#61;:steelblue, ma&#61;0.3,
        xlabel&#61;L&quot;X&quot;, ylabel&#61;L&quot;Y&quot;,
        xlims&#61;&#40;-100, 100&#41;&#41;</code></pre>
<p><img src="/Bayesian-Julia/assets/pages/11_Turing_tricks/code/output/funnel.svg" alt=""> <div class=text-center ><em>Neal&#39;s Funnel</em></div> <br/></p>
<p>Here we see that in upper part of the funnel HMC has to take few steps \(L\) and be more liberal with the \(\epsilon\) factor. But, the opposite is in the lower part of the funnel: way more steps \(L\) and be more conservative with the \(\epsilon\) factor.</p>
<p>To see the devil&#39;s funnel &#40;how it is known in some Bayesian circles&#41; in action, let&#39;s code it in Turing and then sample:</p>
<pre><code class=language-julia >@model funnel&#40;&#41; &#61; begin
    y ~ Normal&#40;0, 3&#41;
    x ~ Normal&#40;0, exp&#40;y / 2&#41;&#41;
end

    chain_funnel &#61; sample&#40;funnel&#40;&#41;, NUTS&#40;&#41;, MCMCThreads&#40;&#41;, 2_000, 4&#41;</code></pre><pre><code class="plaintext code-output">Chains MCMC chain (2000×14×4 Array{Float64, 3}):

Iterations        = 1001:1:3000
Number of chains  = 4
Samples per chain = 2000
Wall duration     = 6.99 seconds
Compute duration  = 12.97 seconds
parameters        = y, x
internals         = lp, n_steps, is_accept, acceptance_rate, log_density, hamiltonian_energy, hamiltonian_energy_error, max_hamiltonian_energy_error, tree_depth, numerical_error, step_size, nom_step_size

Summary Statistics
  parameters      mean       std   naive_se      mcse        ess      rhat   ess_per_sec
      Symbol   Float64   Float64    Float64   Float64    Float64   Float64       Float64

           y    0.8173    2.3536     0.0263    0.2025    33.4392    1.2536        2.5784
           x   -0.1963    8.0061     0.0895    0.3107   707.5545    1.0144       54.5574

Quantiles
  parameters       2.5%     25.0%     50.0%     75.0%     97.5%
      Symbol    Float64   Float64   Float64   Float64   Float64

           y    -3.3093   -0.5414    0.6953    2.1233    5.8291
           x   -10.5991   -0.3819    0.0244    0.8997    8.9652
</code></pre>
<p>Wow, take a look at those <code>rhat</code> values... That sucks: all are above <code>1.01</code> even with 4 parallel chains with 2,000 iterations&#33;</p>
<p>How do we deal with that? We <strong>reparametrize</strong>&#33; Note that we can add two normal distributions in the following manner:</p>
\[ \text{Normal}(\mu, \sigma) = \text{Standard Normal} \cdot \sigma + \mu \]
<p>where the standard normal is the normal with mean \(\mu = 0\) and standard deviation \(\sigma = 1\). This is why is called Non-Centered Parametrization because we &quot;decouple&quot; the parameters and reconstruct them before.</p>
<pre><code class=language-julia >@model ncp_funnel&#40;&#41; &#61; begin
    x̃ ~ Normal&#40;&#41;
    ỹ ~ Normal&#40;&#41;
    y &#61; 3.0 * ỹ         # implies y ~ Normal&#40;0, 3&#41;
    x &#61; exp&#40;y / 2&#41; * x̃  # implies x ~ Normal&#40;0, exp&#40;y / 2&#41;&#41;
end

chain_ncp_funnel &#61; sample&#40;ncp_funnel&#40;&#41;, NUTS&#40;&#41;, MCMCThreads&#40;&#41;, 2_000, 4&#41;</code></pre><pre><code class="plaintext code-output">Chains MCMC chain (2000×14×4 Array{Float64, 3}):

Iterations        = 1001:1:3000
Number of chains  = 4
Samples per chain = 2000
Wall duration     = 6.72 seconds
Compute duration  = 12.3 seconds
parameters        = x̃, ỹ
internals         = lp, n_steps, is_accept, acceptance_rate, log_density, hamiltonian_energy, hamiltonian_energy_error, max_hamiltonian_energy_error, tree_depth, numerical_error, step_size, nom_step_size

Summary Statistics
  parameters      mean       std   naive_se      mcse         ess      rhat   ess_per_sec
      Symbol   Float64   Float64    Float64   Float64     Float64   Float64       Float64

           x̃   -0.0089    1.0121     0.0113    0.0121   7718.0386    0.9998      627.3808
           ỹ   -0.0225    1.0010     0.0112    0.0105   7965.1462    0.9999      647.4676

Quantiles
  parameters      2.5%     25.0%     50.0%     75.0%     97.5%
      Symbol   Float64   Float64   Float64   Float64   Float64

           x̃   -2.0208   -0.6805   -0.0009    0.6672    1.9722
           ỹ   -1.9827   -0.7171   -0.0324    0.6690    1.9056
</code></pre>
<p>Much better now: all <code>rhat</code> are well below <code>1.01</code> &#40;or below <code>0.99</code>&#41;.</p>
<p>How we would implement this a real-world model in Turing? Let&#39;s go back to the <code>cheese</code> random-intercept model in <a href="/Bayesian-Julia/pages/10_multilevel_models/">10. <strong>Multilevel Models &#40;a.k.a. Hierarchical Models&#41;</strong></a>. Here was the approach that we took, also known as Centered Parametrization &#40;CP&#41;:</p>
<pre><code class=language-julia >@model varying_intercept&#40;X, idx, y; n_gr&#61;length&#40;unique&#40;idx&#41;&#41;, predictors&#61;size&#40;X, 2&#41;&#41; &#61; begin
    #priors
    α ~ Normal&#40;mean&#40;y&#41;, 2.5 * std&#40;y&#41;&#41;       # population-level intercept
    β ~ filldist&#40;Normal&#40;0, 2&#41;, predictors&#41;  # population-level coefficients
    σ ~ Exponential&#40;1 / std&#40;y&#41;&#41;             # residual SD
    #prior for variance of random intercepts
    #usually requires thoughtful specification
    τ ~ truncated&#40;Cauchy&#40;0, 2&#41;, 0, Inf&#41;     # group-level SDs intercepts
    αⱼ ~ filldist&#40;Normal&#40;0, τ&#41;, n_gr&#41;       # CP group-level intercepts

    #likelihood
    ŷ &#61; α .&#43; X * β .&#43; αⱼ&#91;idx&#93;
    y ~ MvNormal&#40;ŷ, σ&#41;
end;</code></pre>
<p>To perform a Non-Centered Parametrization &#40;NCP&#41; in this model we do as following:</p>
<pre><code class=language-julia >@model varying_intercept_ncp&#40;X, idx, y; n_gr&#61;length&#40;unique&#40;idx&#41;&#41;, predictors&#61;size&#40;X, 2&#41;&#41; &#61; begin
    #priors
    α ~ Normal&#40;mean&#40;y&#41;, 2.5 * std&#40;y&#41;&#41;       # population-level intercept
    β ~ filldist&#40;Normal&#40;0, 2&#41;, predictors&#41;  # population-level coefficients
    σ ~ Exponential&#40;1 / std&#40;y&#41;&#41;             # residual SD

    #prior for variance of random intercepts
    #usually requires thoughtful specification
    τ ~ truncated&#40;Cauchy&#40;0, 2&#41;, 0, Inf&#41;    # group-level SDs intercepts
    zⱼ ~ filldist&#40;Normal&#40;0, 1&#41;, n_gr&#41;      # NCP group-level intercepts

    #likelihood
    ŷ &#61; α .&#43; X * β .&#43; zⱼ&#91;idx&#93; .* τ
    y ~ MvNormal&#40;ŷ, σ&#41;
end;</code></pre>
<p>Here we are using a NCP with the <code>zⱼ</code>s following a standard normal and we reconstruct the group-level intercepts by multiplying the <code>zⱼ</code>s by <code>τ</code>. Since the original <code>αⱼ</code>s had a prior centered on 0 with standard deviation <code>τ</code>, we only have to use the multiplication by <code>τ</code> to get back the <code>αⱼ</code>s.</p>
<p>Now let&#39;s see how NCP compares to the CP. First, let&#39;s redo our CP hierarchical model:</p>
<pre><code class=language-julia >url &#61; &quot;https://raw.githubusercontent.com/storopoli/Bayesian-Julia/master/datasets/cheese.csv&quot;
cheese &#61; CSV.read&#40;HTTP.get&#40;url&#41;.body, DataFrame&#41;

for c in unique&#40;cheese&#91;:, :cheese&#93;&#41;
    cheese&#91;:, &quot;cheese_&#36;c&quot;&#93; &#61; ifelse.&#40;cheese&#91;:, :cheese&#93; .&#61;&#61; c, 1, 0&#41;
end

cheese&#91;:, :background_int&#93; &#61; map&#40;cheese&#91;:, :background&#93;&#41; do b
    b &#61;&#61; &quot;rural&quot; ? 1 :
    b &#61;&#61; &quot;urban&quot; ? 2 : missing
end

X &#61; Matrix&#40;select&#40;cheese, Between&#40;:cheese_A, :cheese_D&#41;&#41;&#41;;
y &#61; cheese&#91;:, :y&#93;;
idx &#61; cheese&#91;:, :background_int&#93;;

model_cp &#61; varying_intercept&#40;X, idx, y&#41;
chain_cp &#61; sample&#40;model_cp, NUTS&#40;&#41;, MCMCThreads&#40;&#41;, 2_000, 4&#41;</code></pre><pre><code class="plaintext code-output">Chains MCMC chain (2000×21×4 Array{Float64, 3}):

Iterations        = 1001:1:3000
Number of chains  = 4
Samples per chain = 2000
Wall duration     = 28.06 seconds
Compute duration  = 53.61 seconds
parameters        = α, β[1], β[2], β[3], β[4], σ, τ, αⱼ[1], αⱼ[2]
internals         = lp, n_steps, is_accept, acceptance_rate, log_density, hamiltonian_energy, hamiltonian_energy_error, max_hamiltonian_energy_error, tree_depth, numerical_error, step_size, nom_step_size

Summary Statistics
  parameters       mean       std   naive_se      mcse         ess      rhat   ess_per_sec
      Symbol    Float64   Float64    Float64   Float64     Float64   Float64       Float64

           α    70.6389    5.4404     0.0608    0.1406   1242.6163    1.0039       23.1805
        β[1]     3.2471    1.2450     0.0139    0.0208   3626.7256    1.0010       67.6552
        β[2]   -11.5811    1.2609     0.0141    0.0231   3575.6847    1.0006       66.7031
        β[3]     7.1901    1.2402     0.0139    0.0199   3894.2297    1.0003       72.6454
        β[4]     1.2419    1.2261     0.0137    0.0225   3391.0476    1.0005       63.2587
           σ     6.0020    0.2773     0.0031    0.0037   5541.2307    0.9998      103.3696
           τ     6.4554    6.0447     0.0676    0.1421   1765.8541    1.0006       32.9414
       αⱼ[1]    -3.4139    5.3882     0.0602    0.1392   1226.4621    1.0043       22.8792
       αⱼ[2]     3.7646    5.3863     0.0602    0.1375   1224.8120    1.0042       22.8484

Quantiles
  parameters       2.5%      25.0%      50.0%      75.0%     97.5%
      Symbol    Float64    Float64    Float64    Float64   Float64

           α    58.7484    68.4187    70.8060    73.1054   80.7554
        β[1]     0.8025     2.4150     3.2373     4.0810    5.6861
        β[2]   -14.0831   -12.4355   -11.5799   -10.7397   -9.1259
        β[3]     4.7099     6.3697     7.1880     8.0296    9.6526
        β[4]    -1.1378     0.4098     1.2468     2.0583    3.6664
           σ     5.4936     5.8050     5.9960     6.1803    6.5825
           τ     1.9308     3.3224     4.7274     7.2355   21.3945
       αⱼ[1]   -13.6833    -5.7064    -3.4942    -1.2918    8.2770
       αⱼ[2]    -6.2808     1.4520     3.6187     5.8355   15.5226
</code></pre>
<p>Now let&#39;s do the NCP hierarchical model:</p>
<pre><code class=language-julia >model_ncp &#61; varying_intercept_ncp&#40;X, idx, y&#41;
chain_ncp &#61; sample&#40;model_ncp, NUTS&#40;&#41;, MCMCThreads&#40;&#41;, 2_000, 4&#41;</code></pre><pre><code class="plaintext code-output">Chains MCMC chain (2000×21×4 Array{Float64, 3}):

Iterations        = 1001:1:3000
Number of chains  = 4
Samples per chain = 2000
Wall duration     = 30.03 seconds
Compute duration  = 55.02 seconds
parameters        = α, β[1], β[2], β[3], β[4], σ, τ, zⱼ[1], zⱼ[2]
internals         = lp, n_steps, is_accept, acceptance_rate, log_density, hamiltonian_energy, hamiltonian_energy_error, max_hamiltonian_energy_error, tree_depth, numerical_error, step_size, nom_step_size

Summary Statistics
  parameters       mean       std   naive_se      mcse         ess      rhat   ess_per_sec
      Symbol    Float64   Float64    Float64   Float64     Float64   Float64       Float64

           α    70.9355    4.1302     0.0462    0.1276    923.2097    1.0041       16.7786
        β[1]     3.2419    1.2348     0.0138    0.0198   3523.4232    1.0014       64.0355
        β[2]   -11.6155    1.2664     0.0142    0.0225   3356.7947    1.0028       61.0071
        β[3]     7.1725    1.2588     0.0141    0.0227   2721.8062    1.0004       49.4667
        β[4]     1.2165    1.2059     0.0135    0.0200   3557.4821    1.0008       64.6545
           σ     5.9993    0.2649     0.0030    0.0038   5766.2122    1.0009      104.7964
           τ     5.4175    2.9725     0.0332    0.1111    678.2025    1.0019       12.3258
       zⱼ[1]    -0.8522    0.7952     0.0089    0.0157   2529.1240    1.0015       45.9649
       zⱼ[2]     0.8342    0.8003     0.0089    0.0195   1561.9182    1.0011       28.3866

Quantiles
  parameters       2.5%      25.0%      50.0%      75.0%     97.5%
      Symbol    Float64    Float64    Float64    Float64   Float64

           α    62.3884    68.5648    70.8586    73.2044   80.3316
        β[1]     0.7711     2.4166     3.2396     4.0741    5.6360
        β[2]   -14.0885   -12.4646   -11.6158   -10.7833   -9.1017
        β[3]     4.6079     6.3432     7.1826     8.0005    9.6229
        β[4]    -1.1263     0.4203     1.2196     2.0322    3.5609
           σ     5.4923     5.8200     5.9930     6.1703    6.5389
           τ     1.9264     3.2486     4.5757     6.7748   13.1162
       zⱼ[1]    -2.4546    -1.4028    -0.8235    -0.2862    0.6411
       zⱼ[2]    -0.7054     0.2721     0.8180     1.3778    2.4217
</code></pre>
<p>Notice that some models are better off with a standard Centered Parametrization &#40;as is our <code>cheese</code> case here&#41;. While others are better off with a Non-Centered Parametrization. But now you know how to apply both parametrizations in Turing. Before we conclude, we need to recover our original <code>αⱼ</code>s. We can do this by multiplying <code>zⱼ&#91;idx&#93; .* τ</code>:</p>
<pre><code class=language-julia >τ &#61; summarystats&#40;chain_ncp&#41;&#91;:τ, :mean&#93;
αⱼ &#61; mapslices&#40;x -&gt; x * τ, chain_ncp&#91;:, namesingroup&#40;chain_ncp, :zⱼ&#41;, :&#93;.value.data, dims&#61;&#91;2&#93;&#41;
chain_ncp_reconstructed &#61; hcat&#40;MCMCChains.resetrange&#40;chain_ncp&#41;, Chains&#40;αⱼ, &#91;&quot;αⱼ&#91;&#36;i&#93;&quot; for i in 1:length&#40;unique&#40;idx&#41;&#41;&#93;&#41;&#41;</code></pre><pre><code class="plaintext code-output">Chains MCMC chain (2000×23×4 Array{Float64, 3}):

Iterations        = 1:2000
Number of chains  = 4
Samples per chain = 2000
Wall duration     = 30.03 seconds
Compute duration  = 55.02 seconds
parameters        = α, β[1], β[2], β[3], β[4], σ, τ, zⱼ[1], zⱼ[2], αⱼ[1], αⱼ[2]
internals         = lp, n_steps, is_accept, acceptance_rate, log_density, hamiltonian_energy, hamiltonian_energy_error, max_hamiltonian_energy_error, tree_depth, numerical_error, step_size, nom_step_size

Summary Statistics
  parameters       mean       std   naive_se      mcse         ess      rhat   ess_per_sec
      Symbol    Float64   Float64    Float64   Float64     Float64   Float64       Float64

           α    70.9355    4.1302     0.0462    0.1276    923.2097    1.0041       16.7786
        β[1]     3.2419    1.2348     0.0138    0.0198   3523.4232    1.0014       64.0355
        β[2]   -11.6155    1.2664     0.0142    0.0225   3356.7947    1.0028       61.0071
        β[3]     7.1725    1.2588     0.0141    0.0227   2721.8062    1.0004       49.4667
        β[4]     1.2165    1.2059     0.0135    0.0200   3557.4821    1.0008       64.6545
           σ     5.9993    0.2649     0.0030    0.0038   5766.2122    1.0009      104.7964
           τ     5.4175    2.9725     0.0332    0.1111    678.2025    1.0019       12.3258
       zⱼ[1]    -0.8522    0.7952     0.0089    0.0157   2529.1240    1.0015       45.9649
       zⱼ[2]     0.8342    0.8003     0.0089    0.0195   1561.9182    1.0011       28.3866
       αⱼ[1]    -4.6170    4.3081     0.0482    0.0850   2529.1240    1.0015       45.9649
       αⱼ[2]     4.5193    4.3356     0.0485    0.1056   1561.9182    1.0011       28.3866

Quantiles
  parameters       2.5%      25.0%      50.0%      75.0%     97.5%
      Symbol    Float64    Float64    Float64    Float64   Float64

           α    62.3884    68.5648    70.8586    73.2044   80.3316
        β[1]     0.7711     2.4166     3.2396     4.0741    5.6360
        β[2]   -14.0885   -12.4646   -11.6158   -10.7833   -9.1017
        β[3]     4.6079     6.3432     7.1826     8.0005    9.6229
        β[4]    -1.1263     0.4203     1.2196     2.0322    3.5609
           σ     5.4923     5.8200     5.9930     6.1703    6.5389
           τ     1.9264     3.2486     4.5757     6.7748   13.1162
       zⱼ[1]    -2.4546    -1.4028    -0.8235    -0.2862    0.6411
       zⱼ[2]    -0.7054     0.2721     0.8180     1.3778    2.4217
       αⱼ[1]   -13.2976    -7.5995    -4.4612    -1.5506    3.4730
       αⱼ[2]    -3.8217     1.4738     4.4317     7.4644   13.1196
</code></pre>
<h2 id=references ><a href="#references" class=header-anchor >References</a></h2>
<p>Neal, Radford M. &#40;2003&#41;. Slice Sampling. The Annals of Statistics, 31&#40;3&#41;, 705–741. Retrieved from https://www.jstor.org/stable/3448413</p>

<div class=page-foot >
  <div class=copyright >
    Last modified: November 09, 2021. Website built with <a href="https://github.com/tlienart/Franklin.jl">Franklin.jl</a> and the <a href="https://julialang.org">Julia programming language</a>.
  </div>
</div>
</div>
    </div> 
    </div> 
    </div> <!-- end of class page-wrap-->
    
      <script src="/Bayesian-Julia/libs/katex/katex.min.js"></script>
<script src="/Bayesian-Julia/libs/katex/auto-render.min.js"></script>
<script>renderMathInElement(document.body)</script>

    
    
      <script src="/Bayesian-Julia/libs/highlight/highlight.pack.js"></script>
<script>hljs.initHighlightingOnLoad();hljs.configure({tabReplace: '    '});</script>